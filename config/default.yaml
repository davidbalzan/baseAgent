# baseAgent Configuration
# Environment variables are referenced as ${VAR_NAME}

llm:
  # Provider: "openrouter" | "anthropic" | "openai" | "ollama"
  provider: openrouter
  model: google/gemini-2.0-flash-001
  apiKey: ${OPENROUTER_API_KEY}

  # Fallback chain: if primary model fails, try these in order
  # fallbackModels:
  #   - provider: anthropic
  #     model: claude-sonnet-4-20250514
  #   - provider: ollama
  #     model: llama3

  # Provider-specific overrides (used when provider is switched)
  providers:
    anthropic:
      apiKey: ${ANTHROPIC_API_KEY}
    openai:
      apiKey: ${OPENAI_API_KEY}
    ollama:
      baseUrl: http://localhost:11434

channels:
  telegram:
    enabled: true
    token: ${TELEGRAM_BOT_TOKEN}
    allowedUserIds:
      - "1975268993"
  discord:
    enabled: true
    token: ${DISCORD_BOT_TOKEN}

agent:
  maxIterations: 10
  timeoutMs: 120000
  costCapUsd: 1.00

memory:
  compactionThreshold: 4000
  maxTokenBudget: 8000
  toolOutputDecayIterations: 3
  toolOutputDecayThresholdChars: 500

heartbeat:
  enabled: false
  intervalMs: 1800000 # 30 min
  # channelId: "telegram:12345"

governance:
  read: auto-allow
  write: confirm
  exec: confirm
  # toolOverrides:
  #   memory_write: auto-allow
  #   shell_exec: deny

rateLimit:
  channel:
    maxRequests: 10
    windowMs: 60000       # 10 messages/min per user
  http:
    maxRequests: 20
    windowMs: 60000       # 20 requests/min per IP
  tool:
    maxRequests: 50
    windowMs: 60000       # 50 tool calls/min per session

server:
  port: 3000
  host: 0.0.0.0
